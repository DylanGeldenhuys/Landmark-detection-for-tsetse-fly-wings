{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET \n",
    "import pandas as pd\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.transform import rotate as rotate_transform\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rotate(self, image, params):\n",
    "\n",
    "        angle = params['rotation_range'][0]\n",
    "        angle = (random.uniform(0,1))*random.choice([-1,1])*angle\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = rotate_transform(np.array(image), angle = angle, mode = 'edge')\n",
    "\n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "    def translation(self, image,  params):\n",
    "        image_shape = np.array(image).shape\n",
    "        ty = random.uniform(params['height_shift_range'][0]*image_shape[0],          \n",
    "                            params['height_shift_range'][1]*image_shape[0])\n",
    "        tx = random.uniform(params['width_shift_range'][0]*image_shape[1],\n",
    "                            params['width_shift_range'][1]*image_shape[1] )\n",
    "\n",
    "        \n",
    "        horizontal_shift =  tx*random.choice([-1,1])\n",
    "        vertical_shift = ty*random.choice([-1,1])\n",
    "        horizontal_shift_normalised = horizontal_shift/image_shape[1]\n",
    "        vertical_shift_normalised =  vertical_shift/image_shape[0]\n",
    "\n",
    "        transform = AffineTransform(translation=(-horizontal_shift,-vertical_shift))\n",
    "\n",
    "        image = warp(np.array(image),transform,mode='edge')\n",
    "\n",
    "\n",
    "  \n",
    "        # PIL expects RGB images to be uint with ranges from 0 to 255 so we have to convert it to a type that PIL can excpect ie a uint from 0 to 255 \n",
    "        return Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "    def resize(self, image, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image\n",
    "\n",
    "    def zoom(self, image, params):\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        zoom = random.uniform(params['zoom_range'][0],params['zoom_range'][1])\n",
    "        image = TF.resize(image,(int(img_shape[0]*zoom), int(img_shape[1]*zoom)) )\n",
    "        scale_transform = torch.tensor([[zoom, 0], \n",
    "                                        [0, zoom]])\n",
    "\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def color_jitter(self, image):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __call__(self, image, params, image_size):\n",
    "\n",
    "        # set checked image and landmark to landmark_ and image_ (this is for making sure we use the last checked tranformed instead of wrongly tranformed to do the following               # tranform)\n",
    "        \n",
    "        # -----------------------\n",
    "        image_ = Image.fromarray(image.copy())\n",
    "\n",
    "        # -----------------------\n",
    "\n",
    "        # ZOOM\n",
    "        image  = self.zoom(image_,  params)\n",
    "        \n",
    "\n",
    "        # RESIZE\n",
    "\n",
    "        image = self.resize(image, (image_size, image_size))\n",
    "\n",
    "        # ----------------------\n",
    "        #image_, landmarks_ = self.color_jitter(image_, landmarks_)\n",
    "        # ----------------------\n",
    "        \n",
    "        # ROTATE\n",
    "        image = self.rotate(image,  params)\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "        image = image\n",
    "        # ----------------------\n",
    "\n",
    "        # TRANSLATION\n",
    "        image= self.translation(image, params)\n",
    "\n",
    " \n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image\n",
    "\n",
    "class LandmarksDataset():\n",
    "\n",
    "    def __init__(self, transform=None,zoom = [1.0 - 0.03258157476873315, 1.0 + 0.03258157476873315], rotation = [22], height_shift= [0,0.03003200603616672], width_shift= [0,0.03003200603616672 ]):\n",
    "\n",
    "        # targets 0\n",
    "        filenames1 = os.listdir('D:/Tsetse fly Project/Data/Missing_landmarkwings_L/')\n",
    "        #filenames2 = os.listdir('D:/Tsetse fly Project/Data/Missing_landmarkwings_R/')\n",
    "        # targets 1\n",
    "        filenames3 = os.listdir('C:/Users/dylan/Desktop/goodwingsv20-21/')\n",
    "    \n",
    "        self. tranform = transform\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "        self.image_filenames = []\n",
    "        self.targets = []\n",
    "        self.image_size = 244\n",
    "        self.transform = transform\n",
    "        self.image_dir = 'D:/Tsetse fly Project/Data/Missing_landmarkwings_L/'\n",
    "        \n",
    "        #self.image_dir2 = 'D:/Tsetse fly Project/Data/Missing_landmarkwings_R/'\n",
    "        self.image_dir3 = 'C:/Users/dylan/Desktop/goodwingsv20-21/'\n",
    "        self.TransF_ = True\n",
    "\n",
    "       # ------------------- Append left wings data to dataset class ------------\n",
    "\n",
    "        for filename in filenames1:\n",
    "            self.image_filenames.append(os.path.join(self.image_dir, filename))\n",
    "            self.targets.append(1)\n",
    "\n",
    "            \n",
    "\n",
    "        # ------------------ Append flipped right wings data to dataset class-----\n",
    "\n",
    "\n",
    "        #for filename in filenames2[:]:\n",
    "        #    self.targets.append(1)\n",
    "        #    self.image_filenames.append(os.path.join(self.image_dir2, filename))\n",
    "\n",
    "        #num = len(self.targets.copy())\n",
    "        for filename in filenames3:\n",
    "            self.targets.append(0)\n",
    "            self.image_filenames.append(os.path.join(self.image_dir3, filename))\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "\n",
    "    def TransF(self):\n",
    "        self.TransF_ = True\n",
    "    def NoTransF(self):\n",
    "        self.TransF_ = False\n",
    "    def resize(self,size):\n",
    "        self.image_size = size\n",
    "    def set_params(self, zoom = [0.95, 0.105], rotation = [10], height_shift= [0,0.05], width_shift= [0,0.05]):\n",
    "        self.zoom = zoom\n",
    "        self.rotation = rotation\n",
    "        self.height_shift = height_shift\n",
    "        self.width_shift = width_shift\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        params = {'zoom_range': self.zoom, 'rotation_range':self.rotation, 'height_shift_range': self.height_shift, 'width_shift_range': self.width_shift }\n",
    "        image_ = plt.imread(self.image_filenames[index])\n",
    "        target = torch.tensor(self.targets[index])\n",
    "\n",
    "        image = plt.imread(self.image_filenames[index])\n",
    "\n",
    "        \n",
    "        if self.transform and self.TransF_:\n",
    "            \n",
    "            image = self.transform(image_, params, self.image_size)\n",
    "\n",
    "        else:\n",
    "            img_shape = image.copy().shape\n",
    "            image = Image.fromarray(image)\n",
    "            image = TF.resize(image, (self.image_size,self.image_size))\n",
    "       \n",
    "            image = TF.to_tensor(image)\n",
    "            # the following tranform normalises each channel to have a mean at 0.5 and std of 0.5 / NOTE: NOT sure if this is theoreticlly better, should check this\n",
    "            image = TF.normalize(image, [0.5], [0.5])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "DataSet = LandmarksDataset(Transforms())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = 'D:/Tsetse fly Project/Data/test_badgood/'\n",
    "#d_list = os.listdir(d)\n",
    "#g_list = os.listdir('D:/Tsetse fly Project/Data/Missing_landmarks_goodwings/')\n",
    "#for image_name in g_list:\n",
    "#    if image_name not in d_list:\n",
    "#        img = Image.open('D:/Tsetse fly Project/Data/Missing_landmarks_goodwings/' + image_name)\n",
    "#    #img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#        img.save('D:/Tsetse fly Project/Data/Missing_landmarkwings_g/' + image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gw_new = 'D:/Tsetse fly Project/Data/Missing_landmarks_goodwings/'\n",
    "#gw_dir = 'D:/Tsetse fly Project/Data/tsetsedata_2019_left_commas/images_left/'\n",
    "#dir = os.listdir(gw_dir)\n",
    "#for image_name in dir[:555]:\n",
    "#    img = Image.open(gw_dir + image_name)\n",
    "#    img.save(gw_new + image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    return(sum((predictions.round() == y)) / float(len(y))).item()\n",
    "\n",
    "    # helper functions\n",
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.6f \" % (step, total_step, loss))   \n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.6f \" % (step, total_step, loss))\n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Train set is 842\n",
      "The length of Valid set is 280\n",
      "The length of Valid set is 280\n"
     ]
    }
   ],
   "source": [
    "DataSet.TransF()\n",
    "DataSet.resize(299)\n",
    "dataset = DataSet\n",
    "# split the dataset into validation and test sets\n",
    "len_valid_test_set = int(0.2*len(dataset)) # 60% training, 20% validation, 20% testing\n",
    "\n",
    "len_train_set = len(dataset) - len_valid_test_set*2\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_test_set))\n",
    "\n",
    "train_dataset , valid_dataset, test_dataset  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_test_set, len_valid_test_set], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inception_(nn.Module):\n",
    "    def __init__(self,num_classes=1):\n",
    "        super().__init__()\n",
    "        self.model_name='inception'\n",
    "        self.model=models.inception_v3(pretrained=True)\n",
    "        self.model.Conv2d_1a_3x3.conv=nn.Conv2d(3, 32, kernel_size=3, stride=2,  bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.AuxLogits.fc = nn.Linear(self.model.AuxLogits.fc.in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if self.model.training:\n",
    "        \n",
    "            x = torch.sigmoid(x.logits)\n",
    "        else:\n",
    "            \n",
    "            x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Steps: 14/14  Loss: 0.175761 \n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train Loss: 0.3324  Valid Loss: 0.1758\n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train acc: 0.84884  Valid acc: 0.92857\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.1758 at epoch 1/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.086060 \n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train Loss: 0.0774  Valid Loss: 0.0861\n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train acc: 0.97442  Valid acc: 0.96429\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0861 at epoch 2/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.044941 \n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train Loss: 0.0553  Valid Loss: 0.0449\n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train acc: 0.98140  Valid acc: 0.98214\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0449 at epoch 3/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.025586 \n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train Loss: 0.0223  Valid Loss: 0.0256\n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train acc: 0.99651  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0256 at epoch 4/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.046686 \n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train Loss: 0.0192  Valid Loss: 0.0467\n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train acc: 0.99302  Valid acc: 0.98571\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.028897 \n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train Loss: 0.0191  Valid Loss: 0.0289\n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train acc: 0.99767  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.016996 \n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train Loss: 0.0337  Valid Loss: 0.0170\n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train acc: 0.99186  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0170 at epoch 7/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.155402 \n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train Loss: 0.0148  Valid Loss: 0.1554\n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train acc: 0.99535  Valid acc: 0.95357\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.015448 \n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train Loss: 0.0301  Valid Loss: 0.0154\n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train acc: 0.99302  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0154 at epoch 9/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.022724 \n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train Loss: 0.0090  Valid Loss: 0.0227\n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train acc: 0.99767  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.027498 \n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train Loss: 0.0033  Valid Loss: 0.0275\n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train acc: 1.00000  Valid acc: 0.98929\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.012426 \n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train Loss: 0.0057  Valid Loss: 0.0124\n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train acc: 0.99884  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0124 at epoch 12/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.014313 \n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train Loss: 0.0089  Valid Loss: 0.0143\n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train acc: 0.99651  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.004587 \n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train Loss: 0.0147  Valid Loss: 0.0046\n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train acc: 0.99535  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0046 at epoch 14/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.022561 \n",
      "--------------------------------------------------\n",
      "Epoch: 15  Train Loss: 0.0043  Valid Loss: 0.0226\n",
      "--------------------------------------------------\n",
      "Epoch: 15  Train acc: 1.00000  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.050742 \n",
      "--------------------------------------------------\n",
      "Epoch: 16  Train Loss: 0.0079  Valid Loss: 0.0507\n",
      "--------------------------------------------------\n",
      "Epoch: 16  Train acc: 0.99884  Valid acc: 0.98214\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.012001 \n",
      "--------------------------------------------------\n",
      "Epoch: 17  Train Loss: 0.0120  Valid Loss: 0.0120\n",
      "--------------------------------------------------\n",
      "Epoch: 17  Train acc: 0.99767  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.002229 \n",
      "--------------------------------------------------\n",
      "Epoch: 18  Train Loss: 0.0022  Valid Loss: 0.0022\n",
      "--------------------------------------------------\n",
      "Epoch: 18  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 0.0022 at epoch 18/30\n",
      "Model Saved\n",
      "\n",
      "Valid Steps: 14/14  Loss: 0.051181 \n",
      "--------------------------------------------------\n",
      "Epoch: 19  Train Loss: 0.0055  Valid Loss: 0.0512\n",
      "--------------------------------------------------\n",
      "Epoch: 19  Train acc: 0.99767  Valid acc: 0.98929\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.016686 \n",
      "--------------------------------------------------\n",
      "Epoch: 20  Train Loss: 0.0224  Valid Loss: 0.0167\n",
      "--------------------------------------------------\n",
      "Epoch: 20  Train acc: 0.99651  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.029812 \n",
      "--------------------------------------------------\n",
      "Epoch: 21  Train Loss: 0.0071  Valid Loss: 0.0298\n",
      "--------------------------------------------------\n",
      "Epoch: 21  Train acc: 0.99651  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.047898 \n",
      "--------------------------------------------------\n",
      "Epoch: 22  Train Loss: 0.0011  Valid Loss: 0.0479\n",
      "--------------------------------------------------\n",
      "Epoch: 22  Train acc: 1.00000  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.084379 \n",
      "--------------------------------------------------\n",
      "Epoch: 23  Train Loss: 0.0010  Valid Loss: 0.0844\n",
      "--------------------------------------------------\n",
      "Epoch: 23  Train acc: 1.00000  Valid acc: 0.98214\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.082875 \n",
      "--------------------------------------------------\n",
      "Epoch: 24  Train Loss: 0.0007  Valid Loss: 0.0829\n",
      "--------------------------------------------------\n",
      "Epoch: 24  Train acc: 1.00000  Valid acc: 0.97500\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.022007 \n",
      "--------------------------------------------------\n",
      "Epoch: 25  Train Loss: 0.0007  Valid Loss: 0.0220\n",
      "--------------------------------------------------\n",
      "Epoch: 25  Train acc: 1.00000  Valid acc: 0.99643\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.044109 \n",
      "--------------------------------------------------\n",
      "Epoch: 26  Train Loss: 0.0112  Valid Loss: 0.0441\n",
      "--------------------------------------------------\n",
      "Epoch: 26  Train acc: 0.99767  Valid acc: 0.99286\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.058035 \n",
      "--------------------------------------------------\n",
      "Epoch: 27  Train Loss: 0.0043  Valid Loss: 0.0580\n",
      "--------------------------------------------------\n",
      "Epoch: 27  Train acc: 0.99767  Valid acc: 0.98571\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.017605 \n",
      "--------------------------------------------------\n",
      "Epoch: 28  Train Loss: 0.0183  Valid Loss: 0.0176\n",
      "--------------------------------------------------\n",
      "Epoch: 28  Train acc: 0.99535  Valid acc: 0.98929\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.003868 \n",
      "--------------------------------------------------\n",
      "Epoch: 29  Train Loss: 0.0029  Valid Loss: 0.0039\n",
      "--------------------------------------------------\n",
      "Epoch: 29  Train acc: 1.00000  Valid acc: 1.00000\n",
      "--------------------------------------------------\n",
      "Valid Steps: 14/14  Loss: 0.040115 \n",
      "--------------------------------------------------\n",
      "Epoch: 30  Train Loss: 0.0023  Valid Loss: 0.0401\n",
      "--------------------------------------------------\n",
      "Epoch: 30  Train acc: 1.00000  Valid acc: 0.98929\n",
      "--------------------------------------------------\n",
      "Training Complete\n",
      "Total Elapsed Time : 7714.785644292831 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {'acc_train': [], 'acc_val': [], 'loss_train': [], 'loss_val':[], 'time': []}\n",
    "network = inception_().cuda()\n",
    "\n",
    "# TRAIN\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.0001)\n",
    "loss_min = np.inf\n",
    "num_epochs = 30\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    acc_train = 0\n",
    "    acc_valid = 0\n",
    "    running_acc = 0\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "\n",
    "        images, targets = next(iter(train_loader))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            targets = targets.cuda().float() \n",
    "    \n",
    "        predictions = network(images).flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        # find the loss for the current step\n",
    "    \n",
    "        loss_train_step = criterion(predictions, targets)\n",
    "\n",
    "\n",
    "        acc_train_step = accuracy(predictions, targets)\n",
    "\n",
    "\n",
    "        \n",
    "        # calculate the gradients\n",
    "\n",
    "        loss_train_step.backward()\n",
    "    \n",
    "        \n",
    "        # update the parameters\n",
    "\n",
    "        optimizer.step()\n",
    "        acc_train += acc_train_step\n",
    "        loss_train += loss_train_step.item()\n",
    "        \n",
    "        running_acc  = acc_train/step\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            \n",
    "            images, targets = next(iter(valid_loader))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                targets = targets.cuda().float()\n",
    "        \n",
    "            predictions = network(images).flatten()\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, targets)\n",
    "\n",
    "            acc_valid_step = accuracy(predictions, targets)\n",
    "\n",
    "            acc_valid += acc_valid_step\n",
    "            running_acc = acc_valid/step\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    acc_train /= len(train_loader)\n",
    "    acc_valid /= len(valid_loader)\n",
    "    \n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    print('Epoch: {}  Train acc: {:.5f}  Valid acc: {:.5f}'.format(epoch, acc_train, acc_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    results['loss_train'].append(loss_train)\n",
    "    results['loss_val'].append(loss_valid)\n",
    "    results['acc_train'].append(acc_train)\n",
    "    results['acc_val'].append(acc_valid)\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), 'C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/classifiers/models/model_inception_classifer_finetune.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "results['time'].append(time.time()-start_time)\n",
    "del(network)\n",
    "del(images)\n",
    "del(targets)\n",
    "del(predictions)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "f = open(\"C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/classifiers/training_losses/model_inception_classifer_finetune_trainingdata.pkl\",\"wb\")\n",
    "pickle.dump(results,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d67971632b42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# find the loss for the current step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mloss_train_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\Work-Projects\\msc_haar\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\Work-Projects\\msc_haar\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dylan\\Work-Projects\\msc_haar\\env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2481\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2483\u001b[1;33m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[0;32m   2484\u001b[0m         input, target, weight, reduction_enum)\n\u001b[0;32m   2485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "results = {'acc_train': [], 'acc_val': [], 'loss_train': [], 'loss_val':[], 'time': []}\n",
    "network = inception_()\n",
    "for param in network.model.parameters():\n",
    "    param.requires_grad = False\n",
    "in_features_fc = network.model.fc.in_features \n",
    "network.model.fc = nn.Linear(in_features_fc, out_features=1, bias=True)\n",
    "# TRAIN\n",
    "network.cuda()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(network.parameters())\n",
    "loss_min = np.inf\n",
    "num_epochs = 30\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    acc_train = 0\n",
    "    acc_valid = 0\n",
    "    running_acc = 0\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "\n",
    "        images, targets = next(iter(train_loader))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            targets = targets.cuda().float() \n",
    "    \n",
    "        predictions = network(images).flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        # find the loss for the current step\n",
    "    \n",
    "        loss_train_step = criterion(predictions, targets)\n",
    "\n",
    "\n",
    "        acc_train_step = accuracy(predictions, targets)\n",
    "\n",
    "\n",
    "        \n",
    "        # calculate the gradients\n",
    "\n",
    "        loss_train_step.backward()\n",
    "    \n",
    "        \n",
    "        # update the parameters\n",
    "\n",
    "        optimizer.step()\n",
    "        acc_train += acc_train_step\n",
    "        loss_train += loss_train_step.item()\n",
    "        \n",
    "        running_acc  = acc_train/step\n",
    "        running_loss = loss_train/step\n",
    "\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            \n",
    "            images, targets = next(iter(valid_loader))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                targets = targets.cuda().float()\n",
    "        \n",
    "            predictions = network(images).flatten()\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, targets)\n",
    "\n",
    "            acc_valid_step = accuracy(predictions, targets)\n",
    "\n",
    "            acc_valid += acc_valid_step\n",
    "            running_acc = acc_valid/step\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    acc_train /= len(train_loader)\n",
    "    acc_valid /= len(train_loader)\n",
    "    \n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    results['loss_train'].append(loss_train)\n",
    "    results['loss_train'].append(loss_valid)\n",
    "    results['acc_train'].append(acc_train)\n",
    "    results['acc_val'].append(acc_valid)\n",
    "\n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), 'C:/Users/dylan/Work-Projects/msc_haar/manuscript1_exp/models/model_inception_classifer_fixedfeatures.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "results['time'].append(time.time()-start_time)\n",
    "del(network)\n",
    "del(images)\n",
    "del(targets)\n",
    "del(predictions)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "f = open(\"model_inception_classifer_fixedfeatures_trainingdata.pkl\",\"wb\")\n",
    "pickle.dump(results,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "7ec889e55d6cc5b8bc0c6d4c97764aa47dfaa2c194dbff749325c0fada24bc92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
